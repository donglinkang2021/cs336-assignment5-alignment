defaults:
  - val_datasets:
    - competition_math
    - math500
  - _self_

# Hydra settings
hydra:
  run:
    dir: ckpt/eval/runs/${now:%Y-%m-%d}_${now:%H-%M-%S}
  sweep:
    dir: ckpt/eval/multiruns/${now:%Y-%m-%d}_${now:%H-%M-%S}
    subdir: ${hydra.job.num}
  job:
    name: evaluate_math

# Backend: 'vllm' or 'hf'
backend: vllm

# Model configuration
model:
  model_name_or_path: "models/Qwen2.5-Math-1.5B"
  dtype: bfloat16

# Prompt template
prompt_name: "r1_zero"

# Generation parameters
# These are backend-specific.
# For vLLM, these map to SamplingParams.
# For HF, these map to generate() arguments.
generation:
  temperature: 1.0
  top_p: 1.0
  max_tokens: 4096 # For HF, this is max_new_tokens
  stop: ["</answer>"]
  include_stop_str_in_output: true # vLLM specific
  # HF specific generation args can be added here
  # e.g., do_sample: true

num_gpus: 1
num_samples: null

# Output directory for results
output_dir: "outputs/math_evaluation"

# Random seed
seed: 42

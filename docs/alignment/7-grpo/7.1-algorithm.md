## 7.1 GRPO Algorithm

**Advantage estimation.** The core idea of GRPO is to sample many outputs for each question from the policy $\pi_\theta$ and use them to compute a baseline. This is convenient because we avoid the need to learn a neural value function $V_\phi(s)$, which can be hard to train and is cumbersome from the systems perspective. For a question $q$ and group outputs $\{o^{(i)}\}_{i=1}^G \sim \pi_\theta(\cdot|q)$, let $r^{(i)} = R(q, o^{(i)})$ be the reward for the $i$-th output. DeepSeekMath [Shao et al., 2024] and DeepSeek R1 [DeepSeek-AI et al., 2025] compute the group-normalized reward for the $i$-th output as

$$
A^{(i)} = \frac{r^{(i)} - \text{mean}(r^{(1)}, r^{(2)}, \dots, r^{(G)})}{\text{std}(r^{(1)}, r^{(2)}, \dots, r^{(G)}) + \text{advantage\_eps}} \tag{28}
$$

where `advantage_eps` is a small constant to prevent division by zero. Note that this advantage $A^{(i)}$ is the same for each token in the response, i.e., $A_t^{(i)} = A^{(i)}, \forall t \in 1, \dots, |o^{(i)}|$, so we drop the $t$ subscript in the following.

**High-level algorithm.** Before we dive into the GRPO objective, let us first get an idea of the train loop by writing out the algorithm from Shao et al. [2024] in Algorithm 3.[^2]

[^2]: This is a special case of DeepSeekMathâ€™s GRPO with a verified reward function, no KL term, and no iterative update of the reference and reward model.

$$
\begin{array}{l}
\hline
\textbf{Algorithm 3} \text{ Group Relative Policy Optimization (GRPO)} \\
\hline
\textbf{Input:} \text{ initial policy model } \pi_{\theta_{\text{init}}}; \text{ reward function } R; \text{ task questions } D \\
\text{1. policy model } \pi_\theta \leftarrow \pi_{\theta_{\text{init}}} \\
\text{2. \textbf{for} step = 1, \dots, n\_grpo\_steps \textbf{do}} \\
\text{3. \quad Sample a batch of questions } D_b \text{ from } D \\
\text{4. \quad Set the old policy model } \pi_{\theta_{\text{old}}} \leftarrow \pi_\theta \\
\text{5. \quad Sample } G \text{ outputs } \{o^{(i)}\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|q) \text{ for each question } q \in D_b \\
\text{6. \quad Compute rewards } \{r^{(i)}\}_{i=1}^G \text{ for each sampled output } o^{(i)} \text{ by running reward function } R(q, o^{(i)}) \\
\text{7. \quad Compute } A^{(i)} \text{ with group normalization (Eq. 28)} \\
\text{8. \quad \textbf{for} train step = 1, \dots, n\_train\_steps\_per\_rollout\_batch \textbf{do}} \\
\text{9. \quad \quad Update the policy model } \pi_\theta \text{ by maximizing the GRPO-Clip objective (to be discussed, Eq. 29)} \\
\text{10. \quad \textbf{end for}} \\
\text{11. \textbf{end for}} \\
\textbf{Output:} \pi_\theta \\
\hline
\end{array}
$$

**GRPO objective.** The GRPO objective combines three ideas:

1.  Off-policy policy gradient, as in Eq. 27.
2.  Computing advantages $A^{(i)}$ with group normalization, as in Eq. 28.
3.  A clipping mechanism, as in Proximal Policy Optimization (PPO, Schulman et al. [2017]).

The purpose of clipping is to maintain stability when taking many gradient steps on a single batch of rollouts. It works by keeping the policy $\pi_\theta$ from straying too far from the old policy.

Let us first write out the full GRPO-Clip objective, and then we can build some intuition on what the clipping does:

$$
J_{\text{GRPO-Clip}}(\theta) = \mathbb{E}_{q \sim D, \{o^{(i)}\}_{i=1}^G \sim \pi_\theta(\cdot|q)} \left[ \frac{1}{G} \sum_{i=1}^G \frac{1}{|o^{(i)}|} \sum_{t=1}^{|o^{(i)}|} \min \left( \frac{\pi_\theta(o_t^{(i)}|q, o_{<t}^{(i)})}{\pi_{\theta_{\text{old}}}(o_t^{(i)}|q, o_{<t}^{(i)})} A^{(i)}, \text{clip}\left(\frac{\pi_\theta(o_t^{(i)}|q, o_{<t}^{(i)})}{\pi_{\theta_{\text{old}}}(o_t^{(i)}|q, o_{<t}^{(i)})}, 1-\epsilon, 1+\epsilon\right) A^{(i)} \right) \right] \tag{29}
$$

The hyperparameter $\epsilon > 0$ controls how much the policy can change. To see this, we can rewrite the per-token objective in a more intuitive way following Achiam [2018a,b]. Define the function

$$
g(\epsilon, A^{(i)}) = \begin{cases} (1+\epsilon)A^{(i)} & \text{if } A^{(i)} \ge 0 \\ (1-\epsilon)A^{(i)} & \text{if } A^{(i)} < 0 \end{cases} \tag{30}
$$

We can rewrite the per-token objective as

$$
\text{per-token objective} = \min\left(\frac{\pi_\theta(o_t^{(i)}|q, o_{<t}^{(i)})}{\pi_{\theta_{\text{old}}}(o_t^{(i)}|q, o_{<t}^{(i)})} A^{(i)}, g(\epsilon, A^{(i)})\right)
$$

We can now reason by cases. When the advantage $A^{(i)}$ is positive, the per-token objective simplifies to

$$
\text{per-token objective} = \min\left(\frac{\pi_\theta(o_t^{(i)}|q, o_{<t}^{(i)})}{\pi_{\theta_{\text{old}}}(o_t^{(i)}|q, o_{<t}^{(i)})}, 1+\epsilon\right) A^{(i)}
$$

Since $A^{(i)} > 0$, the objective goes up if the action $o_t^{(i)}$ becomes more likely under $\pi_\theta$, i.e., if $\pi_\theta(o_t^{(i)}|q, o_{<t}^{(i)})$ increases. The clipping with `min` limits how much the objective can increase: once $\pi_\theta(o_t^{(i)}|q, o_{<t}^{(i)}) > (1+\epsilon)\pi_{\theta_{\text{old}}}(o_t^{(i)}|q, o_{<t}^{(i)})$, this per-token objective hits its maximum value of $(1+\epsilon)A^{(i)}$. So, the policy $\pi_\theta$ is not incentivized to go very far from the old policy $\pi_{\theta_{\text{old}}}$.

Analogously, when the advantage $A^{(i)}$ is negative, the model tries to drive down $\pi_\theta(o_t^{(i)}|q, o_{<t}^{(i)})$, but is not incentivized to decrease it below $(1-\epsilon)\pi_{\theta_{\text{old}}}(o_t^{(i)}|q, o_{<t}^{(i)})$ (refer to Achiam [2018b] for the full argument).

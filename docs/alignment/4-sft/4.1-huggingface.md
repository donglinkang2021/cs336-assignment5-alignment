## 4.1 Using HuggingFace Models

### Loading a HuggingFace model and tokenizer

To load a HuggingFace model and tokenizer from a local dir (in `bfloat16` and with FlashAttention-2 to save memory), you can use the following starter code:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "/data/a5-alignment/models/Qwen2.5-Math-1.5B",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)
tokenizer = AutoTokenizer.from_pretrained("/data/a5-alignment/models/Qwen2.5-Math-1.5B")
```

### Forward pass

After we’ve loaded the model, we can run a forward pass on a batch of input IDs and get the logits (with the `.logits`) attribute of the output. Then, we can compute the loss between the model’s predicted logits and the actual labels:

```python
input_ids = train_batch["input_ids"].to(device)
labels = train_batch["labels"].to(device)
logits = model(input_ids).logits
loss = F.cross_entropy(..., ...)
```

### Saving a trained model

To save the model to a directory after training is finished, you can use the `.save_pretrained()` function, passing in the path to the desired output directory. Make sure to save under `/data/yourusername` since they can be quite large. We recommend also saving the tokenizer as well (even if you didn’t modify it), just so the model and tokenizer are self-contained and loadable from a single directory.

```python
# Save the model weights
model.save_pretrained(save_directory=output_dir)
tokenizer.save_pretrained(save_directory=output_dir)
```

### Gradient accumulation

Despite loading the model in `bfloat16` and using FlashAttention-2, even an 80GB GPU does not have enough memory to support reasonable batch sizes. To use larger batch sizes, we can use a technique called gradient accumulation. The basic idea behind gradient accumulation is that rather than updating our model weights (i.e., taking an optimizer step) after every batch, we’ll accumulate the gradients over several batches before taking a gradient step. Intuitively, if we had a larger GPU, we should get the same results from computing the gradient on a batch of 32 examples all at once, vs. splitting them up into 16 batches of 2 examples each and then averaging at the end.

Gradient accumulation is straightforward to implement in PyTorch. Recall that each weight tensor has an attribute `.grad` that stores its gradient. Before we call `loss.backward()`, the `.grad` attribute is `None`. After we call `loss.backward()`, the `.grad` attribute contains the gradient. Normally, we’d take an optimizer step, and then zero the gradients with `optimizer.zero_grad()`, which resets the `.grad` field of the weight tensors:

```python
for inputs, labels in data_loader:
    # Forward pass.
    logits = model(inputs)
    loss = loss_fn(logits, labels)

    # Backward pass.
    loss.backward()

    # Update weights.
    optimizer.step()

    # Zero gradients in preparation for next iteration.
    optimizer.zero_grad()
```

To implement gradient accumulation, we’ll just call the `optimizer.step()` and `optimizer.zero_grad()` every `k` steps, where `k` is the number of gradient accumulation steps. We divide the loss by `gradient_accumulation_steps` before calling `loss.backward()` so that the gradients are averaged across the gradient accumulation steps.

```python
gradient_accumulation_steps = 4
for idx, (inputs, labels) in enumerate(data_loader):
    # Forward pass.
    logits = model(inputs)
    loss = loss_fn(logits, labels) / gradient_accumulation_steps

    # Backward pass.
    loss.backward()

    if (idx + 1) % gradient_accumulation_steps == 0:
        # Update weights every `gradient_accumulation_steps` batches.
        optimizer.step()

        # Zero gradients every `gradient_accumulation_steps` batches.
        optimizer.zero_grad()
```

As a result, our effective batch size when training is multiplied by `k`, the number of gradient accumulation steps.
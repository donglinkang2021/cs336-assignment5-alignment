# 4. Supervised Finetuning for MATH

$$
\begin{array}{l}
\hline
\textbf{Algorithm 1: Supervised Finetuning (SFT)} \\
\hline
\textbf{Input:} \text{ initial policy model } \pi_{\theta_{\text{init}}}; \text{ SFT dataset } \mathcal{D} \\
\text{1. Initialize policy model } \pi_\theta \leftarrow \pi_{\theta_{\text{init}}} \\
\text{2. \textbf{for} step = 1, \dots, } n_{\text{sft\_steps}} \text{ \textbf{do}} \\
\text{3. \quad Sample a batch of question-response pairs } \mathcal{D}_b \text{ from } \mathcal{D} \\
\text{4. \quad Compute the cross-entropy loss of the responses given the questions using the model } \pi_\theta \\
\text{5. \quad Update the model parameters } \theta \text{ by taking a gradient step with respect to the cross-entropy loss} \\
\text{6. \textbf{end for}} \\
\textbf{Output:} \pi_\theta \\
\hline
\end{array}
$$


---

**Supervised finetuning for reasoning**

In this section, we will finetune our base model on the MATH dataset (Algorithm 1). As our goal is to improve the modelâ€™s reasoning ability, rather than finetune it to directly predict correct answers, we will finetune it to first generate a chain-of-thought reasoning trace followed by an answer. To this end, we have made available a dataset of such reasoning traces, obtained from DeepSeek R1 (DeepSeek-AI et al., 2025), in `/data/a5-alignment/MATH/sft.jsonl`.

When training a reasoning model in practice, SFT is often used as a warm-start for a second RL finetuning step. There are two main reasons for this. First, SFT requires high-quality annotated data (i.e., with pre-existing reasoning traces), whereas RL requires only the correct answer for feedback. Second, even in settings where annotated data is plentiful, RL can still unlock performance gains by finding better policies than the SFT data. Unfortunately, the models we use are not big enough to show effects when composing SFT and RL, so for this assignment we will treat these two phases separately.
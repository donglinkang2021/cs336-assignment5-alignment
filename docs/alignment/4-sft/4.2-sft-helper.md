## 4.2 SFT Helper Methods

Next, we will implement some helper methods that you will use during SFT and in the later RL experiments.

As a quick note on nomenclature: in the following sections, we will interchangeably refer to a model’s completion given a prompt as an “output”, “completion”, or “response”.

**Tokenizing prompts and outputs.** For each pair of question and target output `(q, o)`, we will tokenize the question and output separately and concatenate them. Then, we can score the log-probabilities of the output with our SFT model (or in later sections, our RL policy). Moreover, we will need to construct a `response_mask`: a boolean mask that is `True` for all tokens in the response, and `False` for all question and padding tokens. We will use this mask in the training loop to ensure that we only compute the loss on the response tokens.

### Problem (tokenize_prompt_and_output): Prompt and output tokenization (2 points)

**Deliverable:** Implement a method `tokenize_prompt_and_output` that tokenizes the question and output separately, concatenates them together, and constructs a `response_mask`. The following interface is recommended:

```python
def tokenize_prompt_and_output(prompt_strs, output_strs, tokenizer):
```
> Tokenize the prompt and output strings, and construct a mask that is 1 for the response tokens and 0 for other tokens (prompt or padding).

- **Args:**
    - `prompt_strs: list[str]`: List of prompt strings.
    - `output_strs: list[str]`: List of output strings.
    - `tokenizer: PreTrainedTokenizer`: Tokenizer to use for tokenization.
- **Returns:**
    - `dict[str, torch.Tensor]`: Let `prompt_and_output_lens` be a list containing the lengths of the tokenized prompt and output strings. Then the returned dictionary should have the following keys:
        - `input_ids`: `torch.Tensor` of shape `(batch_size, max(prompt_and_output_lens) - 1)`: the tokenized prompt and output strings, with the final token sliced off.
        - `labels`: `torch.Tensor` of shape `(batch_size, max(prompt_and_output_lens) - 1)`: shifted input ids, i.e., the input ids without the first token.
        - `response_mask`: `torch.Tensor` of shape `(batch_size, max(prompt_and_output_lens) - 1)`: a mask on the response tokens in the `labels`.

To test your code, implement `adapters.run_tokenize_prompt_and_output`. Then, run the test with `uv run pytest -k test_tokenize_prompt_and_output` and make sure your implementation passes it.

### Logging per-token entropies

When doing RL, it is often useful to keep track of per-token entropies to see if the predictive distribution of the model is becoming (over)confident. We will implement this now and compare how each of our finetuning approaches affects the model’s predictive entropy.

The entropy of a discrete distribution `p(x)` with support `X` is defined as
$$
H(p) = - \sum_{x \in \mathcal{X}} p(x) \log p(x). \quad (1)
$$
Given our SFT or RL model’s logits, we will compute the per-token entropy, i.e., the entropy of each next-token prediction.

### Problem (compute_entropy): Per-token entropy (1 point)

**Deliverable:** Implement a method `compute_entropy` that computes the per-token entropy of next-token predictions. The following interface is recommended:

```python
def compute_entropy(logits: torch.Tensor) -> torch.Tensor:
```
> Get the entropy of the next-token predictions (i.e., entropy over the vocabulary dimension).

- **Args:**
    - `logits: torch.Tensor`: Tensor of shape `(batch_size, sequence_length, vocab_size)` containing unnormalized logits.
- **Returns:**
    - `torch.Tensor`: Shape `(batch_size, sequence_length)`. The entropy for each next-token prediction.

**Note:** you should use a numerically stable method (e.g., using `logsumexp`) to avoid overflow.

To test your code, implement `adapters.run_compute_entropy`. Then run `uv run pytest -k test_compute_entropy` and ensure your implementation passes.

### Getting log-probabilities from a model

Obtaining log-probabilities from a model is a primitive that we will need in both SFT and RL.

For a prefix `x`, an LM producing next-token logits `fθ(x) ∈ R^|V|`, and a label `y ∈ V`, the log-probability of `y` is
$$
\log p_\theta(y|x) = \log[\text{softmax}(f_\theta(x))]_y, \quad (2)
$$
where the notation `[v]_y` denotes the `y`-th element of the vector `v`.

You will want to use a numerically stable method to compute this, and are free to use methods from `torch.nn.functional`. We also suggest including an argument to optionally compute and return token entropies.

### Problem (get_response_log_probs): Response log-probs (and entropy) (2 points)

**Deliverable:** Implement a method `get_response_log_probs` that gets per-token conditional log-probabilities (given the previous tokens) from a causal language model, and optionally the entropy of the model’s next-token distribution. The following interface is recommended:

```python
def get_response_log_probs(
    model: PreTrainedModel,
    input_ids: torch.Tensor,
    labels: torch.Tensor,
    return_token_entropy: bool = False,
) -> dict[str, torch.Tensor]:
```

- **Args:**
    - `model: PreTrainedModel`: HuggingFace model used for scoring (placed on the correct device and in inference mode if gradients should not be computed).
    - `input_ids: torch.Tensor`: shape `(batch_size, sequence_length)`, concatenated prompt + response tokens as produced by your tokenization method.
    - `labels: torch.Tensor`: shape `(batch_size, sequence_length)`, labels as produced by your tokenization method.
    - `return_token_entropy: bool`: If `True`, also return per-token entropy by calling `compute_entropy`.
- **Returns:**
    - `dict[str, torch.Tensor]`:
        - `"log_probs"`: shape `(batch_size, sequence_length)`, conditional log-probabilities `log pθ(xt | x<t)`.
        - `"token_entropy"`: optional, shape `(batch_size, sequence_length)`, per-token entropy for each position (present only if `return_token_entropy=True`).

**Implementation tips:**
- Obtain logits with `model(input_ids).logits`.

To test your code, implement `adapters.run_get_response_log_probs`. Then run `uv run pytest -k test_get_response_log_probs` and ensure the test passes.

### SFT microbatch train step

The loss we minimize in SFT is the negative log-likelihood of the target output given the prompt. To compute this loss, we need to compute the log-probabilities of the target output given the prompt and sum over all tokens in the output, masking the tokens in the prompt and padding tokens.

We will implement a helper function for this, that we will also make use of later during RL.

### Problem (masked_normalize): Masked normalize (1 point)

**Deliverable:** Implement a method `masked_normalize` that sums over tensor elements and normalizes by a constant while respecting a boolean mask. The following interface is recommended:

```python
def masked_normalize(
    tensor: torch.Tensor,
    mask: torch.Tensor,
    normalize_constant: float,
    dim: int | None = None,
) -> torch.Tensor:
```
> Sum over a dimension and normalize by a constant, considering only those elements where `mask == 1`.

- **Args:**
    - `tensor: torch.Tensor`: The tensor to sum and normalize.
    - `mask: torch.Tensor`: Same shape as `tensor`; positions with `1` are included in the sum.
    - `normalize_constant: float`: the constant to divide by for normalization.
    - `dim: int | None`: the dimension to sum along before normalization. If `None`, sum over all dimensions.
- **Returns:**
    - `torch.Tensor`: the normalized sum, where masked elements (`mask == 0`) don’t contribute to the sum.

To test your code, implement `adapters.run_masked_normalize`. Then run `uv run pytest -k test_masked_normalize` and ensure it passes.

### SFT microbatch train step

We are now ready to implement a single microbatch train step for SFT (recall that for a train minibatch, we iterate over many microbatches if `gradient_accumulation_steps > 1`).

### Problem (sft_microbatch_train_step): Microbatch train step (3 points)

**Deliverable:** Implement a single micro-batch update for SFT, including cross-entropy loss, summing with a mask, and gradient scaling. The following interface is recommended:

```python
def sft_microbatch_train_step(
    policy_log_probs: torch.Tensor,
    response_mask: torch.Tensor,
    gradient_accumulation_steps: int,
    normalize_constant: float = 1.0,
) -> tuple[torch.Tensor, dict[str, torch.Tensor]]:
```
> Execute a forward-and-backward pass on a microbatch.

- **Args:**
    - `policy_log_probs`: `(batch_size, sequence_length)`, per-token log-probabilities from the SFT policy being trained.
    - `response_mask`: `(batch_size, sequence_length)`, `1` for response tokens, `0` for prompt/padding.
    - `gradient_accumulation_steps`: Number of microbatches per optimizer step.
    - `normalize_constant`: The constant by which to divide the sum. It is fine to leave this as `1.0`.
- **Returns:**
    - `tuple[torch.Tensor, dict[str, torch.Tensor]]`:
        - `loss`: scalar tensor. The microbatch loss, adjusted for gradient accumulation. We return this so we can log it.
        - `metadata`: Dict with metadata from the underlying loss call, and any other statistics you might want to log.

**Implementation tips:**
- You should call `loss.backward()` in this function. Make sure to adjust for gradient accumulation.

To test your code, implement `adapters.run_sft_microbatch_train_step`. Then run `uv run pytest -k test_sft_microbatch_train_step` and confirm it passes.

### Logging generations in-the-loop

It’s always good practice to do some in-the-loop logging that involves generation from your model, and reasoning SFT/RL is no exception. Write a function `log_generations` that will prompt your model to generate responses for some given prompts (e.g., sampled from the validation set). It’s a good idea to log at least the following for each example:

1. The input prompt.
2. The response generated by the SFT/RL model.
3. The ground-truth answer.
4. The reward information, including format, answer, and total reward.
5. The average token entropy of the response.
6. The average response length, average response length for correct responses, and average response length for incorrect responses.

### Problem (log_generations): Logging generations (1 point)

**Deliverable:** Implement a function `log_generations` that can be used to log generations from your model.

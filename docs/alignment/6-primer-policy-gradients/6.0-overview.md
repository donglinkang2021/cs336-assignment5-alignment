# 6. Primer on Policy Gradients

An exciting new finding in language model research is that performing RL against verified rewards with strong base models can lead to significant improvements in their reasoning capabilities and performance [OpenAI et al., 2024, DeepSeek-AI et al., 2025]. The strongest such open reasoning models, such as *DeepSeek R1* and *Kimi k1.5* [Team et al., 2025], were trained using policy gradients, a powerful reinforcement learning algorithm that can optimize arbitrary reward functions.

We provide a brief introduction to policy gradients for RL on language models below. Our presentation is based closely on a couple great resources which walk through these concepts in more depth: OpenAI’s *Spinning Up in Deep RL* [Achiam, 2018a] and Nathan Lambert’s *Reinforcement Learning from Human Feedback (RLHF) Book* [Lambert, 2024].

## 6.1 Language Models as Policies

A causal language model (LM) with parameters $\theta$ defines a probability distribution over the next token $a_t \in \mathcal{V}$ given the current text prefix $s_t$ (the state/observation). In the context of RL, we think of the next token $a_t$ as an action and the current text prefix $s_t$ as the state. Hence, the LM is a categorical stochastic policy:

$$
a_t \sim \pi_\theta(\cdot|s_t), \quad \pi_\theta(a_t|s_t) = \text{softmax}(f_\theta(s_t))_{a_t} \tag{3}
$$

Two primitive operations will be needed in optimizing the policy with policy gradients:

1.  **Sampling from the policy**: drawing an action $a_t$ from the categorical distribution above;
2.  **Scoring the log-likelihood of an action**: evaluating $\log \pi_\theta(a_t|s_t)$.

Generally, when doing RL with LLMs, $s_t$ is the partial completion/solution produced so far, and each $a_t$ is the next token of the solution; the episode ends when an end-of-text token is emitted, like `<|end_of_text|>`, or `</answer>` in the case of our `r1_zero` prompt.

## 6.2 Trajectories

A (finite-horizon) trajectory is the interleaved sequence of states and actions experienced by an agent:

$$
\tau = (s_0, a_0, s_1, a_1, \dots, s_T, a_T), \tag{4}
$$

where $T$ is the length of the trajectory, i.e., $a_T$ is an end-of-text token or we have reached a maximum generation budget in tokens.

The initial state is drawn from the start distribution, $s_0 \sim \rho_0(s_0)$; in the case of RL with LLMs, $\rho_0(s_0)$ is a distribution over formatted prompts. In general settings, state transitions follow some environment dynamics $s_{t+1} \sim P(\cdot|s_t, a_t)$. In RL with LLMs, the environment is deterministic: the next state is the old prefix concatenated with the emitted token, $s_{t+1} = s_t \Vert a_t$. Trajectories are also called episodes or rollouts; we will use these terms interchangeably.

## 6.3 Rewards and Return

A scalar reward $r_t = R(s_t, a_t)$ judges the immediate quality of the action taken at state $s_t$. For RL on verified domains, it is standard to assign zero reward to intermediate steps and a verified reward to the terminal action:

$$
r_T = R(s_T, a_T) :=
\begin{cases}
1 & \text{if the trajectory } s_T \Vert a_T \text{ matches the ground-truth according to our reward function} \\
0 & \text{otherwise.}
\end{cases}
$$

The return $R(\tau)$ aggregates rewards along the trajectory. Two common choices are finite-horizon undiscounted returns:

$$
R(\tau) := \sum_{t=0}^T r_t, \tag{5}
$$

and infinite-horizon discounted returns:

$$
R(\tau) := \sum_{t=0}^\infty \gamma^t r_t, \quad 0 < \gamma < 1. \tag{6}
$$

In our case, we will use the undiscounted formulation since episodes have a natural termination point (end-of-text or max generation length).

The objective of the agent is to maximize the expected return:

$$
J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)], \tag{7}
$$

leading to the optimization problem:

$$
\theta^* = \arg\max_\theta J(\theta). \tag{8}
$$

## 6.4 Vanilla Policy Gradient

Next, let us attempt to learn policy parameters $\theta$ with gradient ascent on the expected return: $\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\theta_k)$. The core identity that we will use to do this is the REINFORCE policy gradient, shown below.

$$
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \left( \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \right) R(\tau) \right]. \tag{10}
$$

**Deriving the policy gradient.** How did we get this equation? For completeness, we will give a derivation of this identity below. We will make use of a few identities.

1.  The probability of a trajectory is given by:
    $$
    P(\tau|\theta) = \rho_0(s_0) \prod_{t=0}^T P(s_{t+1}|s_t, a_t) \pi_\theta(a_t|s_t). \tag{11}
    $$
    Therefore, the log-probability of a trajectory is:
    $$
    \log P(\tau|\theta) = \log \rho_0(s_0) + \sum_{t=0}^T \left( \log P(s_{t+1}|s_t, a_t) + \log \pi_\theta(a_t|s_t) \right). \tag{12}
    $$

2.  The log-derivative trick:
    $$
    \nabla_\theta P = P \nabla_\theta \log P. \tag{13}
    $$

3.  The environment terms are constant in $\theta$. $\rho_0$, $P(\cdot|\cdot)$ and $R(\tau)$ do not depend on the policy parameters, so:
    $$
    \nabla_\theta \rho_0 = \nabla_\theta P = \nabla_\theta R(\tau) = 0. \tag{14}
    $$

Applying the facts above:

$$
\begin{align}
\nabla_\theta J(\theta) &= \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)] \tag{15} \\
&= \nabla_\theta \sum_\tau P(\tau|\theta) R(\tau) \tag{16} \\
&= \sum_\tau \nabla_\theta P(\tau|\theta) R(\tau) \tag{17} \\
&= \sum_\tau P(\tau|\theta) \nabla_\theta \log P(\tau|\theta) R(\tau) \quad (\text{Log-derivative trick}) \tag{18} \\
&= \mathbb{E}_{\tau \sim \pi_\theta}[\nabla_\theta \log P(\tau|\theta) R(\tau)], \tag{19}
\end{align}
$$

and therefore, plugging in the log-probability of a trajectory and using the fact that the environment terms are constant in $\theta$, we get the vanilla or REINFORCE policy gradient:

$$
\nabla_\theta J(\pi_\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \left( \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) \right) R(\tau) \right]. \tag{20}
$$

Intuitively, this gradient will increase the log probability of every action in a trajectory that has high return, and decrease them otherwise.

**Sample estimate of the gradient.** Given a batch of $N$ rollouts $\mathcal{D} = \{\tau^{(i)}\}_{i=1}^N$ collected by sampling a starting state $s_0^{(i)} \sim \rho_0(s_0)$ and then running the policy $\pi_\theta$ in the environment, we form an unbiased estimator of the gradient as:

$$
\hat{g} = \frac{1}{N} \sum_{i=1}^N \left( \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) \right) R(\tau^{(i)}). \tag{21}
$$

This vector is used in the gradient-ascent update $\theta \leftarrow \theta + \alpha \hat{g}$.

## 6.5 Policy Gradient Baselines

The main issue with vanilla policy gradient is the high variance of the gradient estimate. A common technique to mitigate this is to subtract from the reward a baseline function $b$ that depends only on the state. This is a type of control variate [Ross, 2022]: the idea is to decrease the variance of the estimator by subtracting a term that is correlated with it, without introducing bias.

Let us define the baselined policy gradient as:

$$
B = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) (R(\tau) - b(s_t)) \right]. \tag{22}
$$

As an example, a reasonable baseline is the on-policy value function $V^\pi(s) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)|s_t = s]$, i.e., the expected return if we start at $s_t = s$ and follow the policy $\pi_\theta$ from there. Then, the quantity $(R(\tau) - V^\pi(s_t))$ is, intuitively, how much better the realized trajectory is than expected.

As long as the baseline depends only on the state, the baselined policy gradient is unbiased. We can see this by rewriting the baselined policy gradient as:

$$
B = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau) \right] - \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) b(s_t) \right]. \tag{23}
$$

Focusing on the baseline term, we see that:

$$
\mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) b(s_t) \right] = \sum_{t=0}^T \mathbb{E}_{s_t} \left[ b(s_t) \mathbb{E}_{a_t \sim \pi_\theta(\cdot|s_t)}[\nabla_\theta \log \pi_\theta(a_t|s_t)] \right]. \tag{24}
$$

In general, the expectation of the score function is zero: $\mathbb{E}_{x \sim P_\theta}[\nabla_\theta \log P_\theta(x)] = 0$. Therefore, the expression in Eq. 24 is zero and:

$$
B = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t|s_t) R(\tau) \right] - 0 = \nabla_\theta J(\pi_\theta), \tag{25}
$$

so we conclude that the baselined policy gradient is unbiased. We will later run an experiment to see whether baselining improves downstream performance.

**A note on policy gradient “losses.”** When we implement policy gradient methods in a framework like PyTorch, we will define a so-called policy gradient loss `pg_loss` such that calling `pg_loss.backward()` will populate the gradient buffers of our model parameters with our approximate policy gradient $\hat{g}$. In math, it can be stated as:

$$
\text{pg\_loss} = \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) (R(\tau^{(i)}) - b(s_t^{(i)})). \tag{26}
$$

`pg_loss` is not a loss in the canonical sense—it’s not meaningful to report `pg_loss` on the train or validation set as an evaluation metric, and a good validation `pg_loss` doesn’t indicate that our model is generalizing well. The `pg_loss` is really just some scalar such that when we call `pg_loss.backward()`, the gradients we obtain through backprop are the approximate policy gradient $\hat{g}$.

> When doing RL, you should always log and report train and validation **rewards**. These are the “meaningful” evaluation metrics and what we are attempting to optimize with policy gradient methods.

## 6.6 Off-Policy Policy Gradient

REINFORCE is an on-policy algorithm: the training data is collected by the same policy that we are optimizing. To see this, let us write out the REINFORCE algorithm:

1.  Sample a batch of rollouts $\{\tau^{(i)}\}_{i=1}^N$ from the current policy $\pi_\theta$.
2.  Approximate the policy gradient as $\nabla_\theta J(\pi_\theta) \approx \hat{g} = \frac{1}{N} \sum_{i=1}^N (\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)})) R(\tau^{(i)})$.
3.  Update the policy parameters using the computed gradient: $\theta \leftarrow \theta + \alpha \hat{g}$.

We need to do a lot of inference to sample a new batch of rollouts, only to take just one gradient step. The behavior of an LM generally cannot change significantly in a single step, so this on-policy approach is highly inefficient.

**Off-policy policy gradient.** In off-policy learning, we instead have rollouts sampled from some policy other than the one we are optimizing. Off-policy variants of popular policy gradient algorithms like PPO and GRPO use rollouts from a previous version of the policy $\pi_{\theta_{\text{old}}}$ to optimize the current policy $\pi_\theta$. The off-policy policy gradient estimate is:

$$
\hat{g}_{\text{off-policy}} = \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^T \frac{\pi_\theta(a_t^{(i)}|s_t^{(i)})}{\pi_{\theta_{\text{old}}}(a_t^{(i)}|s_t^{(i)})} \nabla_\theta \log \pi_\theta(a_t^{(i)}|s_t^{(i)}) R(\tau^{(i)}). \tag{27}
$$

This looks like an importance sampled version of the vanilla policy gradient, with reweighting terms $\frac{\pi_\theta(a_t^{(i)}|s_t^{(i)})}{\pi_{\theta_{\text{old}}}(a_t^{(i)}|s_t^{(i)})}$. Indeed, Eq. 27 can be derived by importance sampling and applying an approximation that is reasonable as long as $\pi_\theta$ and $\pi_{\theta_{\text{old}}}$ are not too different: see Degris et al. [2013] for more on this.

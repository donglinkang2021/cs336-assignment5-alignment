# 9 Leaderboard: GRPO on MATH

As the last part of the (mandatory) assignment, you will experiment with approaches to obtain the highest validation rewards possible within 4 hours of training on 2 H100 GPUs.

**Model.** We will continue using the Qwen 2.5 Math 1.5B Base model.

**Dataset.** We will continue using the MATH train and validation dataset available on the cluster at `/data/a5-alignment/MATH/train.jsonl` and `/data/a5-alignment/MATH/validation.jsonl`. You are not allowed to use any other data or do SFT on reasoning chains from stronger models, etc. You must report validation accuracy on the entire validation set (all 5K examples), using the sampling hyperparameters given above (temperature 1.0, max tokens 1024). You are allowed to filter the train set, or design a curriculum over the data, as you desire. You must use the R1-Zero prompt for validation, and during validation, you must use exactly the `r1_zero_reward_fn` reward function provided in the starter code (you are allowed to develop another reward function for use during training if you wish).

**Algorithm.** You are free to tune hyperparameters or change the training algorithm entirely, as long as you do not use any extraneous data or another model (you are free to use more copies of the model if you want).

**Systems optimizations.** You might observe that in our simple GRPO implementation above, at least one GPU is always idle. You will likely find notable improvements by improving the systems characteristics of our pipeline. For example, you might consider lower precision for rollouts or training, `torch.compile`, and other systems optimizations. You are definitely not constrained to placing vLLM on a single device and the train policy on another device, and are encouraged to think of better ways to parallelize.

**Ideas.** For some ideas on possible improvements, see the following repos:

-   veRL
-   trl
-   torchtune
-   oat

**On KL divergence.** We also note that in the above experiments, we did not include a KL divergence term with respect to some reference model (usually this is a frozen SFT or pretrained checkpoint). In our experiments and others from the literature [Liu et al., 2025, NTT123, 2025], we found that omitting the KL term had no impact on performance while saving GPU memory (no need to store a reference model). However, many GRPO repos include it by default and you are encouraged to experiment with KL or other forms of regularization, as long as you use Qwen 2.5 Math 1.5B Base or some model obtained through your algorithm for it.

**Problem (leaderboard): Leaderboard (16 points) (16 H100 hrs)**

**Deliverable:** Report a validation accuracy obtained within 4 hours of training on 2 H100 GPUs and a screenshot of your validation accuracy with respect to wall-clock time, where the x-axis ends at â‰¤4 hours. As a reminder, we place the following constraints on your evaluation:

1.  Your validation accuracy should be the average accuracy over the entire MATH validation set (all 5K examples).
2.  You must use the R1-Zero prompt at validation time.
3.  You must use temperature 1.0 and max tokens 1024 with vLLM for evaluation.
4.  You must calculate validation accuracy by averaging the answer rewards produced by the `r1_zero_reward_fn` reward function provided in the starter code.

# 1 Assignment Overview

In this assignment, you will gain some hands-on experience with training language models to reason when solving math problems.

**What you will implement.**

1.  Zero-shot prompting baseline for the MATH dataset of competition math problems Hendrycks et al. [2021].
2.  Supervised finetuning, given reasoning traces from a stronger reasoning model (DeepSeek R1, DeepSeek-AI et al. 2025).
3.  Expert Iteration for improving reasoning performance with verified rewards.
4.  Group-Relative Policy Optimization (GRPO) for improving reasoning performance with verified rewards.

For those interested, we will have an entirely optional part of the assignment on aligning language models to human preferences, which will be released in the next few days.

**What you will run.**

1.  Measure Qwen 2.5 Math 1.5B zero-shot prompting performance (our baseline).
2.  Run SFT on Qwen 2.5 Math 1.5B with reasoning traces from R1.
3.  Run Expert Iteration on Qwen 2.5 Math 1.5B with verified rewards.
4.  Run GRPO on Qwen 2.5 Math 1.5B with verified rewards.

**What the code looks like.** All the assignment code as well as this writeup are available on GitHub at: `github.com/stanford-cs336/assignment5-alignment`

Please `git clone` the repository. If there are any updates, we will notify you and you can `git pull` to get the latest.

1.  `cs336_alignment/*`: This is where you’ll write your code for assignment 5. Note that there’s no code in here (aside from a little starter code), so you should be able to do whatever you want from scratch.
2.  `cs336_alignment/prompts/*`: For your convenience, we’ve provided text files with prompts to minimize possible errors caused by copying-and-pasting prompts from the PDF to your code.
3.  `tests/*.py`: This contains all the tests that you must pass. You are only expected to pass the tests in `tests/test_sft.py` and `tests/test_grpo.py`—the rest of the tests are for the non-mandatory parts of the assignment. These tests invoke the hooks defined in `tests/adapters.py`. You’ll implement the adapters to connect your code to the tests. Writing more tests and/or modifying the test code can be helpful for debugging your code, but your implementation is expected to pass the original provided test suite.
4.  `README.md`: This file contains some basic instructions on setting up your environment.

**What you can use.** We expect you to build most of the RL related components from scratch. You may use tools like `vLLM` to generate text from language models (§3.1). In addition, you may use HuggingFace Transformers to load the Qwen 2.5 Math 1.5B model and tokenizer and run forward passes (§4.1), but you may not use any of the training utilities (e.g., the `Trainer` class).

**How to submit.** You will submit the following files to Gradescope:

*   `writeup.pdf`: Answer all the written questions. Please typeset your responses.
*   `code.zip`: Contains all the code you’ve written.

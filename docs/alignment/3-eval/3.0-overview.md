# 3 Measuring Zero-Shot MATH Performance

We’ll start by measuring the performance of our base language model on the 5K example test set of MATH. Establishing this baseline is useful for understanding how each of the later approaches affects model behavior.

Unless otherwise specified, for experiments on MATH we will use the following prompt from the DeepSeek R1-Zero model [DeepSeek-AI et al., 2025]. We will refer to this as the `r1_zero` prompt:

> A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within `<think> </think>` and answer is enclosed within `<answer> </answer>` tags, respectively, i.e., `<think>` reasoning process here `</think> <answer>` answer here `</answer>`.
>
> User: {question}
>
> Assistant: `<think>`

The `r1_zero` prompt is located in the text file `cs336_alignment/prompts/r1_zero.prompt`.

In the prompt, `question` refers to some question that we insert (e.g., *Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?*). The expectation is that the model plays the role of the assistant, and starts generating the thinking process (since we have already included a left think tag `<think>`), closes the thinking process with `</think>` and then generates a final symbolic answer within the answer tags, like `<answer> 4x + 10 </answer>`. The purpose of having the model generate tags like `<answer> </answer>` is so that we can easily parse the model’s output and compare it against a ground truth answer, and so that we can stop response generation when we see the right answer tag `</answer>`.

**Note on prompt choice.** It turns out that the `r1_zero` prompt is not the best choice for maximizing downstream performance after RL, because of a mismatch between the prompt and how the Qwen 2.5 Math 1.5B model was pretrained. Liu et al. [2025] finds that simply prompting the model with the question (and nothing else) starts with a very high accuracy, e.g., matching the `r1_zero` prompt after 100+ steps of RL. Their findings suggest that Qwen 2.5 Math 1.5B was already pretrained on such question-answer pairs. Nonetheless, we choose the `r1_zero` prompt for this assignment because RL with it shows clear accuracy improvements in a short number of steps, allowing us to walk through the mechanics of RL and sanity check correctness quickly, even if we don’t manage the best final performance. As a reality check, you will compare directly to the `question_only` prompt later in the assignment.

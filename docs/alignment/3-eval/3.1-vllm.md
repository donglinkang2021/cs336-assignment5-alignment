## 3.1 Using vLLM for offline language model inference

To evaluate our language models, we’re going to have to generate continuations (responses) for a variety of prompts. While one could certainly implement their own functions for generation (e.g., as you did in assignment 1), efficient implementation of RL requires high-performance inference techniques, and implementing these inference techniques are beyond the scope of this assignment. Therefore, in this assignment we will recommend using vLLM for offline batched inference. vLLM is a high-throughput and memory-efficient inference engine for language models that incorporates a variety of useful efficiency techniques (e.g., optimized CUDA kernels, PagedAttention for efficient attention KV caching [Kwon et al., 2023], etc.). To use vLLM to generate continuations for a list of prompts:[^1]

```python
from vllm import LLM, SamplingParams

# Sample prompts.
prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]

# Create a sampling params object, stopping generation on newline.
sampling_params = SamplingParams(
    temperature=1.0, top_p=1.0, max_tokens=1024, stop=["\n"]
)

# Create an LLM.
llm = LLM(model=<path_to_model>)

# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

[^1]: Example taken from https://github.com/vllm-project/vllm/blob/main/examples/offline_inference.py.

In the example above, the `LLM` can be initialized with the name of a HuggingFace model (which will be automatically downloaded and cached if it isn’t found locally), or a path to a HuggingFace model. Since downloads can take a long time (especially for larger models, e.g., 70B parameters) and to conserve cluster disk space (so everyone doesn’t have their own independent copy of the pre-trained models), we have downloaded the following pre-trained models at the following the paths on the Together cluster. Please do not re-download these models on the Together cluster:

*   **Qwen 2.5 Math 1.5B Base** (for reasoning experiments):
    `/data/a5-alignment/models/Qwen2.5-Math-1.5B`
*   **Llama 3.1 8B Base** (for optional instruction tuning experiments):
    `/data/a5-alignment/models/Llama-3.1-8B`
*   **Llama 3.3 70B Instruct** (for optional instruction tuning experiments):
    `/data/a5-alignment/models/Llama-3.3-70B-Instruct`

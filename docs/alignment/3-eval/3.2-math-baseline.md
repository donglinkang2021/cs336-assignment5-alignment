## 3.2 Zero-shot MATH Baseline

**Prompting setup.** To evaluate zero-shot performance on the MATH test set, we’ll simply load the examples and prompt the language model to answer the question using the `r1_zero` prompt from above.

**Evaluation metric.** When we evaluate a multiple-choice or binary response task, the evaluation metric is clear—we test whether the model outputs exactly the correct answer.

In math problems we assume that there is a known ground truth (e.g. `0.5`) but we cannot simply test whether the model outputs exactly `0.5`—it can also answer `<answer> 1/2 </answer>`. Because of this, we must address the tricky problem of matching for semantically equivalent responses from the LM when we evaluate MATH.

To this end, we want to come up with some answer parsing function that takes as input the model’s output and a known ground-truth, and returns a boolean indicating whether the model’s output is correct. For example, a reward function could receive the model’s string output ending in `<answer> She sold 15 clips. </answer>` and the gold answer `72`, and return `True` if the model’s output is correct and `False` otherwise (in this case, it should return `False`).

For our MATH experiments, we will use a fast and fairly accurate answer parser used in recent work on reasoning RL [Liu et al., 2025]. This reward function is implemented at `cs336_alignment.drgrpo_grader.r1_zero_reward_fn`, and you should use it to evaluate performance on MATH unless otherwise specified.

**Generation hyperparameters.** When generating responses, we’ll sample with temperature `1.0`, top-p `1.0`, max generation length `1024`. The prompt asks the model to end its answer with the string `</answer>`, and therefore we can direct vLLM to stop when the model outputs this string:

```python
# Based on Dr. GRPO: stop when the model completes its answer
# https://github.com/sail-sg/understand-r1-zero/blob/
# c18804602b85da9e88b4aeeb6c43e2f08c594fbc/train_zero_math.py#L167
sampling_params.stop = ["</answer>"]
sampling_params.include_stop_str_in_output = True
```

**Problem (math_baseline): 4 points**

1.  Write a script to evaluate Qwen 2.5 Math 1.5B zero-shot performance on MATH. This script should (1) load the MATH validation examples from `/data/a5-alignment/MATH/validation.jsonl`, (2) format them as string prompts to the language model using the `r1_zero` prompt, and (3) generate outputs for each example. This script should also (4) calculate evaluation metrics and (5) serialize the examples, model generations, and corresponding evaluation scores to disk for analysis in subsequent problems.

    It might be helpful for your implementation to include a method `evaluate_vllm` with arguments similar to the following, as you will be able to reuse it later:

    ```python
    def evaluate_vllm(
        vllm_model: LLM,
        reward_fn: Callable[[str, str], dict[str, float]],
        prompts: List[str],
        eval_sampling_params: SamplingParams
    ) -> None:
        """
        Evaluate a language model on a list of prompts,
        compute evaluation metrics, and serialize results to disk.
        """
    ```

    **Deliverable**: A script to evaluate baseline zero-shot MATH performance.

2.  Run your evaluation script on Qwen 2.5 Math 1.5B. How many model generations fall into each of the following categories: (1) correct with both format and answer reward 1, (2) format reward 1 and answer reward 0, (3) format reward 0 and answer reward 0? Observing at least 10 cases where format reward is 0, do you think the issue is with the base model’s output, or the parser? Why? What about in (at least 10) cases where format reward is 1 but answer reward is 0?

    **Deliverable**: Commentary on the model and reward function performance, including examples of each category.

3.  How well does the Qwen 2.5 Math 1.5B zero-shot baseline perform on MATH?

    **Deliverable**: 1-2 sentences with evaluation metrics.

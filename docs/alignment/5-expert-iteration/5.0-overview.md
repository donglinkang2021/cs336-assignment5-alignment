# 5. Expert Iteration for MATH

In the previous section, we observed that we can improve the performance of our SFT model by filtering out bad examples from the SFT data. In this section, we will go one step further: we will apply this filtering procedure to reasoning traces we generate from our base model itself. This process is known in the literature as expert iteration [Anthony et al., 2017], and in the context of language models has been explored in Cobbe et al. [2021b], Zelikman et al. [2022], Dohan et al. [2022], Gulcehre et al. [2023].

$$
\begin{array}{l}
\hline
\textbf{Algorithm 2: Expert Iteration (EI)} \\
\hline
\textbf{Input:} \text{ initial policy model } \pi_{\theta_{\text{init}}}; \text{ reward function } R; \text{ task questions } D \\
\text{1. policy model } \pi_\theta \leftarrow \pi_{\theta_{\text{init}}} \\
\text{2. \textbf{for} step = 1, \dots, n\_ei\_steps \textbf{do}} \\
\text{3. \quad Sample a batch of questions } D_b \text{ from } D \\
\text{4. \quad Set the old policy model } \pi_{\theta_{\text{old}}} \leftarrow \pi_\theta \\
\text{5. \quad Sample } G \text{ outputs } \{o^{(i)}\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|q) \text{ for each question } q \in D_b \\
\text{6. \quad Compute rewards } \{r^{(i)}\}_{i=1}^G \text{ for each sampled output } o^{(i)} \text{ by running reward function } R(q, o^{(i)}) \\
\text{7. \quad Filter out wrong outputs (i.e., } o^{(i)} \text{ with } r^{(i)} = 0 \text{) to obtain a dataset } D_{\text{sft}} \text{ of correct question-response pairs} \\
\text{8. \quad } \pi_\theta \leftarrow \text{SFT}(\pi_\theta, D_{\text{sft}}) \text{ (Algorithm 1)} \\
\text{9. \textbf{end for}} \\
\textbf{Output:} \pi_\theta \\
\hline
\end{array}
$$

Next, we will run expert iteration on the MATH dataset.

As a tip, you should pass a `min_tokens` value to your vLLM `SamplingParams`, which will ensure that you do not generate an empty string (which could then cause a NaN downstream depending on your implementation). This can be done with:

```python
sampling_min_tokens = 4
sampling_params = SamplingParams(
    temperature=sampling_temperature,
    max_tokens=sampling_max_tokens,
    min_tokens=sampling_min_tokens,
    n=G,
    seed=seed,
)
```

As in SFT, you should use gradient clipping with a clip value of `1.0`.

## Problem: Expert Iteration Experiment (2 points, 6 H100 hrs)

Run expert iteration on the MATH dataset (provided at `/data/a5-alignment/MATH/train.jsonl`) using the Qwen 2.5 Math 1.5B Base model, varying the number of rollouts `G` per question and the number of epochs used in the SFT step, and using `n_ei_steps = 5`. Vary the batch size for each expert iteration step (i.e., the size of $D_b$) in `{512, 1024, 2048}`. (You do not need to try all possible combinations of these hyperparameters. Just enough to draw conclusions about each is fine.) Log the entropy of the model’s reponses over training. Make sure to have vLLM terminate generations at the second answer tag `</answer>`, as done in the SFT section.

- **Deliverable:** Validation accuracy curves associated with different rollout configurations. Try at least 2 different rollout counts and epoch counts.
- **Deliverable:** A model that achieves validation accuracy of at least 15% on MATH.
- **Deliverable:** A brief 2 sentence discussion comparing to your SFT performance, as well as performance across EI steps.
- **Deliverable:** A plot of the entropy of the model’s responses over training.

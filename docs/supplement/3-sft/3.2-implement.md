## 3.2 Implementing instruction fine-tuning

Now that we’ve taken a look at our instruction-tuning data, let’s implement the pieces we’ll need for instruction fine-tuning.

### 3.2.1 Data Loader

Our instruction fine-tuning dataset is a collection of `(prompt, response)` pairs. To fine-tune our language models on this data, we need to convert these `(prompt, response)` pairs to strings. We’ll use the following template from Alpaca:

```
Below is an instruction that describes a task. Write a response that appropriately
completes the request.

### Instruction:
{prompt}

### Response:
{response}
```

We can treat these strings as documents for language modeling and train our models on this data. As with other types of data, we concatenate all of these documents into a single sequence of tokens, adding a delimiter between them (e.g., the Llama 3.1 8B base uses the `<|end_of_text|>` token).

A data loader turns this sequence of tokens into a stream of batches, where each batch consists of `B` sequences of length `m`, paired with the corresponding next tokens, also with length `m`. In practice, examples are often “packed” into constant-length sequences, which minimizes padding tokens and thus maximizes GPU throughput. To break our sequence of tokens into chunks of length `m`, we’ll take consecutive, non-overlapping chunks of size `m` (dropping the final chunk if has fewer than `m` tokens). For example, given a sequence token IDs `[0, 1, 2, ..., 9, 10]` with a desired sequence length of 4, we’d have potential batch inputs `[[0, 1, 2, 3], [4, 5, 6, 7]]`. Iterating over the data loader should return each of these inputs exactly once, which constitutes an epoch over our data.

**Problem (data_loading): Implement data loading (3 points)**

**(a) Deliverable:** Implement a PyTorch `Dataset` subclass that generates examples for instruction tuning. The `Dataset` should have the following interface:

`def __init__(self, tokenizer, dataset_path, seq_length, shuffle)`
Constructs the dataset. `tokenizer` is a `transformers` tokenizer for use in tokenizing and encoding the instruction tuning data. `dataset_path` is a path to instruction tuning data. `seq_length` is the desired length of sequences to generate from the dataset (typically the desired language model context length). `shuffle` controls whether or not documents are shuffled before concatenation (when `shuffle=True`), or if they are concatenated in the order they appear in the data (when `shuffle=False`).

`def __len__(self)`
Returns an integer, the number of sequences in this `Dataset`. For example, given a sequence token IDs `[0, 1, 2, ..., 9, 10]` with a desired sequence length of 4, the `Dataset` would have length 2 (`len([[0, 1, 2, 3], [4, 5, 6, 7]])`).

`def __getitem__(self, i)`
Returns the `i`th element of the `Dataset`. `i` must be less than the length of the `Dataset` returned by `__len__(self)`. This function should return a dictionary with at least the following keys:
- `input_ids`: a PyTorch tensor of shape `(seq_length, )` with the input token IDs for the `i`th example.
- `labels`: a PyTorch tensor of shape `(seq_length, )` with the token IDs of the corresponding labels for the `i`th example.

To test your implementation against our provided tests, you will first need to implement the test adapter at `[adapters.get_packed_sft_dataset]`. Then, run `uv run pytest -k test_packed_sft_dataset` to test your implementation.

**(b) Deliverable:** Implement a function that returns batches from your previously-implemented `Dataset`. Your function should accept as input (1) a dataset to take batches from, (2) the desired batch size, and (3) whether or not to shuffle the examples before batching them up. Iterating through these batches should constitute a single epoch through the data. You may find `torch.utils.data.DataLoader` to be useful.

To test your implementation against our provided tests, you will first need to implement the test adapter at `[adapters.run_iterate_batches]`. Then, run `uv run pytest -k test_iterate_batches` to test your implementation.

### 3.2.2 Training script

Now that we’ve implemented a data loader for our instruction fine-tuning data, we’ll write a training script to fine-tune a pre-trained Llama 3.1 8B base model.

If you did the required part of Assignment 5, this is precisely the same approach to load and use HuggingFace models (as well as to perform gradient accumulation), but we re-print it here for convenience.

**Loading the model for fine-tuning.** To load the Llama 3.1 8B base model, we’ll use HuggingFace `transformers`. We’ll load the model in `bfloat16` format and use FlashAttention-2 to save memory. To load the model and tokenizer for training:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(
    model_name_or_path,
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)
```

**Computing language modeling loss.** After we’ve loaded the model, we can run a forward pass on a batch of input IDs and get the logits (with the `.logits`) attribute of the output. Then, we can compute the loss between the model’s predicted logits and the actual labels:

```python
input_ids = train_batch["input_ids"].to(device)
labels = train_batch["labels"].to(device)
logits = model(input_ids).logits
loss = F.cross_entropy(..., ...)
```

**Saving the trained model.** To save the model to a directory after training is finished, you can use the `.save_pretrained()` function, passing in the path to the desired output directory. We recommend also saving the tokenizer as well (even if you didn’t modify it), just so the model and tokenizer are self-contained and loadable from a single directory.

```python
# Save the model weights
model.save_pretrained(save_directory=output_dir)
tokenizer.save_pretrained(save_directory=output_dir)
```

**Gradient accumulation.** Despite loading the model in `bfloat16` and using FlashAttention-2, even an 80GB GPU does not have enough memory to support reasonable batch sizes. With the setup above, you should be able to train the model on sequences of 512 tokens with a batch size of 2 sequences per batch. However, we’d prefer to use a larger batch size (e.g., 32 sequences per batch). To accomplish this, we can use a technique called gradient accumulation. The basic idea behind gradient accumulation is that rather than updating our model weights (i.e., taking an optimizer step) after every batch, we’ll accumulate the gradients over several batches before taking a gradient step. Intuitively, if we had a larger GPU, we should get the same results from computing the gradient on a batch of 32 examples all at once, vs. splitting them up into 16 batches of 2 examples each and then averaging at the end.

Gradient accumulation is straightforward to implement in PyTorch. Recall that each weight tensor has an attribute `.grad` that stores its gradient. Before we call `loss.backward()`, the `.grad` attribute is `None`. After we call `loss.backward()`, the `.grad` attribute contains the gradient. Normally, we’d take an optimizer step, and then zero the gradients with `optimizer.zero_grad()`, which resets the `.grad` field of the weight tensors:

```python
for inputs, labels in data_loader:
    # Forward pass.
    logits = model(inputs)
    loss = loss_fn(logits, labels)

    # Backward pass.
    loss.backward()

    # Update weights.
    optimizer.step()

    # Zero gradients in preparation for next iteration.
    optimizer.zero_grad()
```

To implement gradient accumulation, we’ll just call the `optimizer.step()` and `optimizer.zero_grad()` every `k` steps, where `k` is the number of gradient accumulation steps. We divide the loss by `gradient_accumulation_steps` before calling `loss.backward()` to so that the gradients are averaged across the gradient accumulation steps.

```python
gradient_accumulation_steps = 4
for idx, (inputs, labels) in enumerate(data_loader):
    # Forward pass.
    logits = model(inputs)
    loss = loss_fn(logits, labels) / gradient_accumulation_steps

    # Backward pass.
    loss.backward()

    if (idx + 1) % gradient_accumulation_steps == 0:
        # Update weights every `gradient_accumulation_steps` batches.
        optimizer.step()

        # Zero gradients every `gradient_accumulation_steps` batches.
        optimizer.zero_grad()
```

As a result, our effective batch size when training is multiplied by `k`, the number of gradient accumulation steps.

**Problem (sft_script): Training script: instruction tuning (4 points)**

**Deliverable:** Write a script that runs a training loop fine-tune the Llama 3.1 8B base model on the provided instruction tuning data. In particular, we recommend that your training script allow for (at least) the following:

- Ability to configure and control the various model and optimizer hyperparameters.
- Ability to train on larger batch sizes than can fit in memory via gradient accumulation.
- Periodically logging training and validation performance (e.g., to console and/or an external service like Weights and Biases).[^1]

If you’ve completed the previous assignments (e.g., A1, mandatory part of A5), feel free to adapt the training script that you previously wrote to support fine-tuning pre-trained language models on instruction tuning data with gradient accumulation. Alternatively, you may find the provided training script from assignment 4 to be a useful starting point (though we encourage you to write the script from scratch if you haven’t already done it).[^2]

[^1]: wandb.ai
[^2]: https://github.com/stanford-cs336/spring2024-assignment4-data/blob/master/cs336-basics/scripts/train.py

Now that we’ve written our training script, let’s instruction tune our base model.

**Problem (sft): Instruction Tuning (6 points) (24 H100 hrs)**

Fine-tune Llama 3 8B base on the provided instruction tuning data. We recommend training single epoch using a context length of 512 tokens with a total batch size of 32 sequences per gradient step.[^3] Make sure to save your model and tokenizer after training, since we’ll evaluate their performance and also use them later in the assignment for further post-training on preference pairs. We used a learning rate of 2e-5 with cosine decay and a linear warmup (3% of total training steps), but it may be useful to experiment with different learning rates to get a better intuition for what values work well.

**Deliverable:** A description of your training setup, along with the final validation loss that was recorded and an associated learning curve. In addition, make sure to serialize the model and tokenizer after training for use in the next parts of the assignment.

[^3]: We were able to use a batch size of 2 when training the model in `bfloat16` and using FlashAttention-2.

## 2.1 Zero-shot MMLU baseline

**Prompting setup.** To evaluate zero-shot performance on MMLU, we’ll load the examples and prompt the language model to answer the multiple choice question. Since the language model simply outputs free-form text, it isn’t always trivial to evaluate its outputs. In this case, naively prompting the language model with our system prompt and an MMLU example means that it’ll sometimes output the letter corresponding to the correct answer, the text of the correct answer, or even a paraphrased version of the correct answer. These variations can make it complex to parse the answer from the model’s generations. As a result, properly evaluating these models often requires specifying a particular answer format in the prompt. In the case of MMLU, we’ll use the following prompt (in conjunction with the system prompt above):

```markdown
Answer the following multiple choice question about {subject}. Respond with a single sentence of the form "The correct answer is _", filling the blank with the letter corresponding to the correct answer (i.e., A, B, C or D).

Question: {question}
A. {options[0]}
B. {options[1]}
C. {options[2]}
D. {options[3]}
Answer:
```

In the prompt above, `{subject}` refers to the subject split of the MMLU example (e.g., high school geography), the `{question}` is the question text (e.g., *Which of the following is a centrifugal force in a country?*), and `{options}` is a list of the multiple-choice options for this question (i.e., `["Religious differences", "A national holiday", "An attack by another country", "A charismatic national leader"]`).

**Evaluation metric.** To evaluate the model’s outputs, we’ll parse its generations into the letter of the corresponding predicted answer (i.e., “A”, “B”, “C”, or “D”). Then, we can compare the letter of the predicted answer with the letter of the gold answer to assess whether or not the model answered the question correctly.

**Generation hyperparameters.** When generating responses, we’ll use greedy decoding (i.e., temperature of 0.0, with top-p 1.0).

**Problem (mmlu_baseline): 4 points**

1.  **(a)** Write a function to parse generated language model outputs into the letter corresponding to the predicted answer. If model response cannot be parsed, return `None`. To test your function, implement the adapter `[run_parse_mmlu_response]` and make sure it passes `uv run pytest -k test_parse_mmlu_response`.

    *Deliverable: A function to parse generated predictions on MMLU into the letter of the corresponding answer option.*

2.  **(b)** Write a script to evaluate Llama 3.1 8B zero-shot performance on MMLU. This script should (1) load the MMLU examples, (2) format them as string prompts to the language model, and (3) generate outputs for each example. This script should also (4) calculate evaluation metrics and (5) serialize the examples, model generations, and corresponding evaluation scores to disk for further analysis.

    *Deliverable: A script to evaluate baseline zero-shot MMLU performance.*

3.  **(c)** Run your evaluation script on Llama 3.1 8B. How many model generations does your evaluation function fail to parse? If non-zero, what do these examples look like?

    *Deliverable: Number of model generations that failed parsing. If non-zero, a few examples of generations that your function wasn’t able to parse.*

4.  **(d)** How long does it take the model to generate responses to each of the MMLU examples? Estimate the throughput in examples/second.

    *Deliverable: Estimate of MMLU examples/second throughput.*

5.  **(e)** How well does the Llama 3.1 8B zero-shot baseline perform on MMLU?

    *Deliverable: 1-2 sentences with evaluation metrics.*

6.  **(f)** Sample 10 random incorrectly-predicted examples from the evaluation dataset. Looking through the examples, what sort of errors does the language model make?

    *Deliverable: A 2-4 sentence error analysis of model predictions, including examples and/or model responses as necessary.*

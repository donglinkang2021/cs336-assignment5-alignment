## 2.3 AlpacaEval

**Prompting setup.** To evaluate zero-shot performance on AlpacaEval, we’ll simply load the examples and prompt the language model with the instruction; no other prompting should be necessary, since the instructions themselves are already well-defined inputs:

```
{instruction}
```

In the prompt above, `instruction` refers to an AlpacaEval prompt (e.g., *What are the names of some famous actors that started their careers on Broadway?*).

**Evaluation metric.** To evaluate the model’s outputs on each instruction, we’ll prompt an annotator model (typically a stronger and/or larger model) to assess whether it prefers our model’s generated output or the generated output of a reference model. A model’s winrate against a given reference model is the proportion of model outputs that are preferred over the reference model, with respect to some annotator model.

We’ll compare our model’s outputs against GPT-4 Turbo (the default reference model in AlpacaEval), and we’ll use Llama 3.3 70B Instruct as our annotator to compute our model’s winrate.

**Generation hyperparameters.** When generating responses, we’ll use greedy decoding (i.e., temperature of 0.0, with top-p 1.0).

### Problem (alpaca_eval_baseline): 4 points

(a) Write a script to collect Llama 3.1 8B zero-shot predictions on AlpacaEval. This script should (1) load the AlpacaEval instructions, (2) generate outputs for each instruction, and (3) serialize the outputs and model generations to disk for evaluation. For compatibility with the AlpacaEval evaluator, your output predictions must be serialized as a JSON array. Each entry of this JSON array should contain a JSON object with the following keys:
*   `instruction`: the instruction.
*   `output`: the output of the model, given the instruction.
*   `generator`: a string identifier corresponding to the name of the model that generated the output (e.g., `llama-3.1-8b-base`). This should be the same across all entries in the JSON array.
*   `dataset`: a string identifier that indicates which dataset the instruction comes from. This is provided in the original AlpacaEval dataset.

As an example, assuming that `eval_set` is a list of AlpacaEval examples, you can compute outputs like so:

```python
for example in eval_set:
    # generate here is a placeholder for your models generations
    example["output"] = generate(example["instruction"])
    example["generator"] = "my_model" # name of your model

with open("output.json", "w") as fout:
    json.dump(eval_set, fout)
```

**Deliverable:** A script to generate zero-shot outputs on AlpacaEval.

(b) How long does it take the model to generate responses to each of the AlpacaEval examples? Estimate the throughput in examples/second.

**Deliverable:** Estimate of AlpacaEval examples/second throughput.

(c) To measure our model’s performance on AlpacaEval, we’ll use Llama 3.3 70B Instruct as the annotator and compare our outputs against GPT-4 Turbo. To compute the winrate, run the following command (requires two GPUs, each with more than 80GB of memory):

```bash
uv run alpaca_eval --model_outputs <path_to_model_predictions.json> \
--annotators_config 'scripts/alpaca_eval_vllm_llama3_3_70b_fn' \
--base-dir '.'
```

This command will load our model outputs and run Llama 3.3 70B Instruct locally to get its preference judgments and compute the corresponding winrate. What is the winrate and length-controlled winrate of our zero-shot baseline model when compared against GPT-4 Turbo and using Llama 3.3 70B Instruct as the annotator?

**Deliverable:** 1-2 sentences with the winrate and length-controlled winrate.

(d) Sample 10 random examples where the baseline model’s response is dispreferred versus GPT-4 Turbo (you should be able to see the AlpacaEval annotations at `scripts/alpaca_eval_vllm_llama3_3_70b_fn/annotations_seed0_configs.json`). Looking through the examples, why do you think the baseline model is dispreferred? Are there any cases where you disagree with the automatic evaluator?

**Deliverable:** A 2-4 sentence error analysis of model predictions, including examples and/or model responses as necessary.

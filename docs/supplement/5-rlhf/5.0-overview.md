# 5. "Reinforcement Learning" from "Human Feedback"

During SFT, we train our model to imitate responses from a given set of high-quality examples. Still, that is often not enough to mitigate undesired behavior from a language model that was learned during pre-training. While SFT relies on an external set of good examples, for aligning language models it is often helpful to elicit responses from the model itself that we are trying to improve, and reward or penalize those responses based on some assessment of their quality and appropriateness.

A method that gained popularity recently for its use in the OpenAI models was **Reinforcement Learning from Human Feedback**, or RLHF [Ouyang et al., 2022]. In RLHF, we start with a set of prompts to be given to our model after SFT. Then, we elicit sets of responses from the model to each prompt. The “Reinforcement Learning” part of RLHF comes from the fact that we do not get a per-token loss (as we have in SFT, since in that setting we are given a reference response), but instead train the model to optimize for a scalar reward signal that measures how appropriate a (complete) response is for a given prompt. The “HF” indicates that, at least in the original method, this reward signal was obtained from fitting a model on data from human annotators, who manually ranked the given sets of responses.

The original RLHF method is fairly intricate. After SFT, we first generate $K$ responses for each prompt, and have humans rank them (which is an expensive step to do at scale). Then, RLHF proposes to explicitly fit a reward model, $r_\theta(x, y)$, which assigns a scalar reward for a response $y$ being given to a prompt $x$. Here, $r_\theta$ starts as the SFT model with the final (output) layer removed, and an extra layer that outputs a scalar value. Then, we’ll sample prompts $x$ and pairs of responses $y_w, y_l$ from the human preferences dataset (where $y_w$ was ranked better than $y_l$), and optimize the following loss:

$$
\ell_{r_\theta}(x, y_w, y_l) = -\log \sigma(r_\theta(x, y_w) - r_\theta(x, y_l)) \quad (1)
$$

where $\sigma$ is the sigmoid function. Intuitively, we want the reward model to output scalar rewards that agree with the rankings from the human annotators; $\ell_r$ is lower the higher the agreement is between $r$ and the human data. After having a reward model, RLHF proceeds by optimizing the LM using RL, where we see the LM as a policy $\pi_\theta$ that is given a prompt and chooses a token to generate at each step, until it finishes its response (completing an RL “episode”), at which point it gets a reward given by $r_\theta$. The original paper used Proximal Policy Optimization (PPO) for training the LM using the reward model. Additionally, the paper describing RLHF on GPT-3 models found it important to:

1.  Add a KL-divergence penalty to prevent the model from deviating too much from the SFT model.
2.  Have an auxiliary loss function using the pre-training (language modeling) objective, to avoid degenerating performance on downstream tasks.

RLHF has many moving parts, and has been reportedly difficult to reproduce besides the success that OpenAI had applying it. More recently, another method for aligning models with preference data, named **Direct Preference Optimization** (DPO; Rafailov et al., 2023), has become widely popular, for both its simplicity and effectiveness, producing models that often perform on par or better than models trained with RLHF. In the last part of this assignment, you will implement DPO and experiment with using it to align models using datasets of preference labels.

## 5.4 DPO Training

You will now implement a training loop using DPO on the HH data. Unlike during SFT, we now have to run two examples through the LMs ($\pi_{\text{ref}}$ and $\pi_{\theta}$) to compute the loss, which takes significant GPU memory. Thus, we will not try to batch our implementation, and use gradient accumulation, as done in SFT, to allow for larger effective batch sizes. Similarly, we won’t be able to use AdamW unless we use other eﬀiciency tricks (such as quantization), so we will stick to the RMSprop optimizer (`torch.optim.RMSprop`), as also done in the original DPO work. We suggest the following implementation path, which sacrifices maximal performance for simplicity:

*   Use 2 GPUs, one for the reference model, and one for the trained model.
*   Load two copies of your instruction fine-tuned model, one in each device.
*   Separate out a small number of examples (e.g., 200) as a validation set.
*   Train your model with DPO loss and gradient accumulation, tracking your loss at each step.
*   We recommend you start with a batch size of 64, $\beta = 0.1$, and a learning rate of $1e-6$.

Besides these tricks, we ask you to keep track of the “classification accuracy” of the implicit reward model on the validation set. This simply amounts to comparing the log-probability of chosen and rejection completions (consider an example to be correctly classified when the chosen completion has higher log-probability).

### Problem (dpo_training): 4 points

1.  Implement your DPO training loop, and train your instruction-tuned Llama 3.1 8B model for 1 epoch over HH. Save your model with the highest validation accuracy.

    **Deliverable**: A script to train your instruction-tuned Llama model with DPO on HH, and a screenshot of your validation accuracy curve during training.

2.  Now, evaluate your model after DPO on AlpacaEval, as you did in problem `alpaca_eval_sft`. What is the new winrate and length-controlled winrate of your DPO-trained model when compared against GPT-4 Turbo, with Llama 3.3 70B Instruct as the annotator? How does that compare to the SFT model you started with?

    **Deliverable**: A 1-2 sentence response with the AlpacaEval winrates of your DPO-trained model.

3.  Evaluate your DPO-trained model on SimpleSafetyTests. How does it compare to the SFT model?

    **Deliverable**: A 1-2 sentence response with your SimpleSafetyTests evaluation.

4.  Both AlpacaEval and SimpleSafetyTests test behaviours that are directly demonstrated in HH, such as instruction following and refusing potentally harmful prompts. Past work in alignment of language models, including the Anthropic paper introducing HH, have often observed an “alignment tax”, where aligned models might also lose some of their capabilities. Evaluate your DPO model on GSM8k and MMLU. What do you observe?

    **Deliverable**: A 2-3 sentence response with your evaluations on GSM8k and MMLU.

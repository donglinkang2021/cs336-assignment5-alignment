## 5.3 Implementing the DPO loss

We’ll now start our implementation of DPO, which we’ll use to align our LM using the preference datasets we looked at. You will now implement the per-instance DPO loss, given in Equation 3, given a pair of LMs (the LM we’re optimizing and the reference model), and a pair of responses to the same prompt $x$ (the preferred response $y_w$ and the rejected response $y_l$). Note that, as we’re dealing with large models, the two models you receive might not be in the same device. You should return the loss in the same device as the LM we’re optimizing, accounting for the fact that the reference model might be in another device.

**Problem (dpo_loss): 2 points**

Write a function that computes the per-instance DPO loss. Your function will receive two language models, and two strings containing both the better and worse responses according to the preference dataset. Use the Alpaca template (the same we used for SFT) to format the prompt and responses you are given, and make sure to add the “end of sequence” token after the response. To simplify your implementation, you can use the following observation: when computing a difference of conditional log-probabilities under the same model (e.g., $\log \pi_\theta (y_w |x) - \log \pi_\theta (y_l|x)$), this is equivalent to computing the difference of the unconditional log-probabilities (e.g., $\log \pi_\theta (x \oplus y_w ) - \log \pi_\theta (x \oplus y_l)$, where $\oplus$ denotes the concatenation of sequences of tokens), since the log-probability of the prompt cancels out.

**Deliverable:** A function that takes two LMs ($\pi_\theta$ and $\pi_{\text{ref}}$), a tokenizer, and two strings (the prompt concatenated with both a chosen response $y_w$ and a rejected response $y_l$), and computes the per-instance DPO loss. Implement the adapter `adapters.per_instance_dpo` and make sure it passes `uv run pytest -k test_per_instance_dpo_loss`.

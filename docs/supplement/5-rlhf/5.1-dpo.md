## 5.1 The DPO objective

In RLHF, we first explicitly fit a reward model `rθ` using the collected preference data, and then optimize the LM to generate completions that receive high reward. DPO starts from the observation that, instead of first (a) finding an optimal reward model `r` that agrees with the preference data, and then (b) finding an optimal policy `πr` for that reward model, we can instead derive a reparameterization of the optimal reward model that can be represented in terms of the optimal policy itself:

$$
r(x, y) = \beta \log \frac{\pi_r(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x) \quad (2)
$$

Here, `πref` is the “reference policy”: the original LM after SFT that we don’t want to deviate much from, and `β` is a hyperparameter controlling the strength of the penalty for deviating from `πref`. `πr` is the optimal policy for the reward model `r` – essentially, the optimal LM according to this model. Note that the second term here only depends on a instruction-dependent normalization constant (the partition function `Z(x)`), but not on the completion `y`.

Now, notice that the original per-instance loss in RLHF, in Equation 1, only depends on the difference between rewards assigned to different completions. When we take the difference, the partition function cancels out, and we arrive at the simpler per-instance loss for DPO:

$$
\ell_{\text{DPO}}(\pi_\theta, \pi_{\text{ref}}, x, y_w, y_l) = -\log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \quad (3)
$$

Note that, to compute this loss, we do not need to sample completions during the alignment process, as in RLHF. All we need is to compute conditional log-probabilities; so here there is no explicit “reinforcement learning” happening. Also, the preference data need not necessarily come from human annotators either — several works have had success applying these methods on preferences generated by other language models, often prompted to judge pairs of alternative responses to the same query on a list of given criteria. Thus, this process also doesn’t necessarily involve human feedback.

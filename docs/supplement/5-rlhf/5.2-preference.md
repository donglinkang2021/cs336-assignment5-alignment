## 5.2 Looking at preference data

Before using preference data to align our LMs, it is useful, as always, to look at the data yourself and make sense of it. We will first use both prompts and completions from the HH dataset (“Helpful and Harmless”) collected by Anthropic. We will leverage the training set of 4 collections examples in the dataset, obtained using many different human-written prompts: `harmless-base`, `helpful-online`, `helpful-base` and `helpful-rejection-sampled`. The HH dataset can be downloaded from Hugging Face (https://huggingface.co/datasets/Anthropic/hh-rlhf/tree/main), and we also provide it on the Together cluster, under `/data/a5-alignment/hh`:

```bash
c-nband@ad12a3ca-04:$ ls /data/a5-alignment/hh
harmless-base.jsonl.gz  helpful-base.jsonl.gz
helpful-online.jsonl.gz helpful-rejection-sampled.jsonl.gz
```

These are only the training splits. Each of these gzipped files are in the “JSON lines” format, where each line contains a valid JSON object. You will now write a function to load this dataset, and then manually inspect the data.

### Problem (look_at_hh): 2 points

1.  Write a function to load the Anthropic HH dataset. Make a combined training set containing all of the examples in the 4 files above. After unzipped, each line in these files contains a JSON object with a “chosen” conversation between a human and the assistant (preferred by the human annotator) and a “rejected” conversation, both starting from the same prompt.

    To simplify our use of the dataset for DPO, you should apply the following processing steps:
    *   Ignore multi-turn conversations, e.g. where the human sent more than one message (since those can also diverge in the human messages, beyond the original prompt)
    *   Separate each example into an “instruction” (the first human message) and a pair of chosen and rejected responses (the corresponding messages from the assistant in each case).
    *   Remember which file each example came from (for the analysis below).

    **Deliverable**: A Python function that loads the dataset in a convenient data structure for you to use it for training. The Python modules `gzip` and `json` will be useful.

2.  The Anthropic researchers purposefully did not try to define “helpful” or “harmless”, but instead left that up to the human annotators to interpret. Look at 3 random examples of “helpful” and 3 of “harmless” conversations. Comment on these examples: what seems to be the main differences between the chosen and rejected responses? Do you agree with the annotators choices?

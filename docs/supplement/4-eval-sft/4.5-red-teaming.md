## 4.5 Red-teaming our instruction-tuned model

Red-teaming is an evaluation method that attempts to elicit undesirable and/or unsafe model behaviors to better understand how they fail and how we might improve them [Ganguli et al., 2022]. In this part of the assignment, we’ll try to interactively get a sense of how diﬀicult it is to use our language model for malicious purposes (e.g., assisting users with dangerous activities like making a bomb or creating malware).

**Problem (red_teaming): 4 points**

1.  Beyond the examples listed above, what are three other possible ways that language models might be misused?

    > **Deliverable:** 1-3 sentences with three examples (beyond those presented above) about potential misuses of language models.

2.  Try prompting your fine-tuned language model to assist you in completing three different potentially malicious applications. For each malicous application, provide a description of your methodology and the results, as well as any qualitative takeaways you drew from the experience. For example, your descriptions should answer questions like whether you were successful or unsuccessful, how long you tried to break the model, and strategies that you employed.

    > **Deliverable:** For three different malicious applications, provide a 2-4 sentence description of your red-teaming procedure and results.

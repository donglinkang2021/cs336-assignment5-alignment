## 4.4 SimpleSafetyTests

**Problem (sst_sft): 4 points**

1.  Write a script to collect the predictions of your fine-tuned model on SimpleSafetyTests. How long does it take the model to generate responses to each of the SimpleSafetyTests examples? Estimate the throughput in examples/second, and compare to our previously-used baseline model.

    > **Deliverable:** 1-2 sentences with an estimate of SimpleSafetyTests examples/second throughput and a comparison to the baseline model.

2.  To measure our model’s performance on SimpleSafetyTests, we’ll use Llama 3.3 70B Instruct to annotate responses as being safe or unsafe. To compute the proportion of safe outputs (as judged by Llama 3.3 70B Instruct), run the following command (requires two GPUs, each with more than 80GB of memory):

    ```bash
    uv run python scripts/evaluate_safety.py \
    --input-path <path_to_model_predictions.jsonl> \
    --model-name-or-path /data/a5-alignment/models/Llama-3.3-70B-Instruct \
    --num-gpus 2 \
    --output-path <path_to_write_output.jsonl>
    ```

    This command will load our model outputs and run Llama 3.3 70B locally to get annotations and compute the corresponding proportion of “safe” outputs. What proportion of model outputs are judged as safe? How does this compare to the zero-shot baseline?

    > **Deliverable:** 1-2 sentences with the proportion of safe model outputs (as judged by Llama 3.3 70B Instruct).

3.  Sample 10 random examples where your fine-tuned model’s response is judged to be unsafe (you should be able to see the annotations at the output path that you specified when running the evaluator). Looking through the examples, in what sorts of cases does the model produce unsafe outputs? Are there any cases where you disagree with the automatic evaluator?

    > **Deliverable:** A 2-4 sentence error analysis of model predictions, including examples and/or model responses as necessary.